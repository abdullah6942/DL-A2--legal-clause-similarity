{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "tZ_ugih4r19C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86GxVKlmppw8",
        "outputId": "6765be98-ff17-46c9-b8c4-5162911e6996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GOOGLE COLAB SETUP\n",
            "============================================================\n",
            "âœ“ Running in Google Colab\n",
            "\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Google Drive mounted successfully\n",
            "\n",
            "============================================================\n",
            "Looking for archive folder at:\n",
            "  /content/drive/MyDrive/DL_dataset/archive\n",
            "============================================================\n",
            "âœ“ Archive folder found with 395 CSV files\n",
            "\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "\n",
            "============================================================\n",
            "LEGAL CLAUSE SIMILARITY DETECTION SYSTEM\n",
            "Deep Learning Assignment 2 - Google Colab Version\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LOADING LEGAL CLAUSE DATASET\n",
            "============================================================\n",
            "Data directory: /content/drive/MyDrive/DL_dataset/archive\n",
            "Found 395 CSV files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading CSV files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 395/395 [00:03<00:00, 110.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Loaded 150881 clauses\n",
            "âœ“ Number of unique clause types: 395\n",
            "\n",
            "Building vocabulary...\n",
            "âœ“ Vocabulary size: 30002\n",
            "  Most common words: ['the', 'of', 'or', 'and', 'to', 'any', 'in', 'shall', 'be', 'by']\n",
            "\n",
            "============================================================\n",
            "GENERATING CLAUSE PAIRS\n",
            "============================================================\n",
            "âœ“ Generated 20000 clause pairs\n",
            "  Positive pairs (similar): 10000\n",
            "  Negative pairs (dissimilar): 10000\n",
            "\n",
            "============================================================\n",
            "SPLITTING DATA\n",
            "============================================================\n",
            "âœ“ Training set: 14000 pairs\n",
            "âœ“ Validation set: 3000 pairs\n",
            "âœ“ Test set: 3000 pairs\n",
            "\n",
            "============================================================\n",
            "INITIALIZING MODELS\n",
            "============================================================\n",
            "\n",
            "1. BiLSTM Siamese Network initialized\n",
            "   Parameters: 6,766,081\n",
            "\n",
            "2. Attention-based Encoder initialized\n",
            "   Parameters: 4,137,857\n",
            "\n",
            "============================================================\n",
            "TRAINING BiLSTM-Siamese\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] | Train Loss: 0.1581 | Train Acc: 0.9361 | Val Loss: 0.0618 | Val Acc: 0.9787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20] | Train Loss: 0.0593 | Train Acc: 0.9815 | Val Loss: 0.0485 | Val Acc: 0.9870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20] | Train Loss: 0.0446 | Train Acc: 0.9856 | Val Loss: 0.0430 | Val Acc: 0.9880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20] | Train Loss: 0.0368 | Train Acc: 0.9880 | Val Loss: 0.0403 | Val Acc: 0.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20] | Train Loss: 0.0288 | Train Acc: 0.9911 | Val Loss: 0.0439 | Val Acc: 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20] | Train Loss: 0.0255 | Train Acc: 0.9923 | Val Loss: 0.0383 | Val Acc: 0.9913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20] | Train Loss: 0.0189 | Train Acc: 0.9938 | Val Loss: 0.0400 | Val Acc: 0.9937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20] | Train Loss: 0.0154 | Train Acc: 0.9953 | Val Loss: 0.0472 | Val Acc: 0.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20] | Train Loss: 0.0145 | Train Acc: 0.9958 | Val Loss: 0.0497 | Val Acc: 0.9890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning rate reduced: 0.001000 â†’ 0.000500\n",
            "Epoch [10/20] | Train Loss: 0.0170 | Train Acc: 0.9940 | Val Loss: 0.0563 | Val Acc: 0.9890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20] | Train Loss: 0.0102 | Train Acc: 0.9966 | Val Loss: 0.0512 | Val Acc: 0.9897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/20] | Train Loss: 0.0072 | Train Acc: 0.9975 | Val Loss: 0.0574 | Val Acc: 0.9887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20] | Train Loss: 0.0065 | Train Acc: 0.9980 | Val Loss: 0.0572 | Val Acc: 0.9900\n",
            "Early stopping triggered after epoch 13\n",
            "\n",
            "âœ“ Training completed! Best validation loss: 0.0383\n",
            "\n",
            "============================================================\n",
            "TRAINING Attention-Encoder\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] | Train Loss: 0.6468 | Train Acc: 0.6259 | Val Loss: 0.5880 | Val Acc: 0.6847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20] | Train Loss: 0.5554 | Train Acc: 0.7149 | Val Loss: 0.5017 | Val Acc: 0.7450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20] | Train Loss: 0.4789 | Train Acc: 0.7701 | Val Loss: 0.4709 | Val Acc: 0.7780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20] | Train Loss: 0.4099 | Train Acc: 0.8151 | Val Loss: 0.4221 | Val Acc: 0.8060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20] | Train Loss: 0.3477 | Train Acc: 0.8469 | Val Loss: 0.4077 | Val Acc: 0.8237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20] | Train Loss: 0.2970 | Train Acc: 0.8776 | Val Loss: 0.4048 | Val Acc: 0.8370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20] | Train Loss: 0.2554 | Train Acc: 0.8954 | Val Loss: 0.3888 | Val Acc: 0.8517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20] | Train Loss: 0.2239 | Train Acc: 0.9099 | Val Loss: 0.3851 | Val Acc: 0.8587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20] | Train Loss: 0.2036 | Train Acc: 0.9203 | Val Loss: 0.4165 | Val Acc: 0.8597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20] | Train Loss: 0.1753 | Train Acc: 0.9310 | Val Loss: 0.4333 | Val Acc: 0.8613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/20] | Train Loss: 0.1505 | Train Acc: 0.9423 | Val Loss: 0.4652 | Val Acc: 0.8713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning rate reduced: 0.001000 â†’ 0.000500\n",
            "Epoch [12/20] | Train Loss: 0.1417 | Train Acc: 0.9445 | Val Loss: 0.4868 | Val Acc: 0.8777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/20] | Train Loss: 0.1153 | Train Acc: 0.9578 | Val Loss: 0.5030 | Val Acc: 0.8830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20] | Train Loss: 0.0943 | Train Acc: 0.9649 | Val Loss: 0.5311 | Val Acc: 0.8797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20] | Train Loss: 0.0832 | Train Acc: 0.9714 | Val Loss: 0.5279 | Val Acc: 0.8853\n",
            "Early stopping triggered after epoch 15\n",
            "\n",
            "âœ“ Training completed! Best validation loss: 0.3851\n",
            "\n",
            "============================================================\n",
            "EVALUATING BiLSTM-Siamese\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.9930\n",
            "Precision: 0.9920\n",
            "Recall:    0.9940\n",
            "F1-Score:  0.9930\n",
            "ROC-AUC:   0.9990\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1488   12]\n",
            " [   9 1491]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Dissimilar       0.99      0.99      0.99      1500\n",
            "     Similar       0.99      0.99      0.99      1500\n",
            "\n",
            "    accuracy                           0.99      3000\n",
            "   macro avg       0.99      0.99      0.99      3000\n",
            "weighted avg       0.99      0.99      0.99      3000\n",
            "\n",
            "\n",
            "============================================================\n",
            "EVALUATING Attention-Encoder\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.8463\n",
            "Precision: 0.8411\n",
            "Recall:    0.8540\n",
            "F1-Score:  0.8475\n",
            "ROC-AUC:   0.9276\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1258  242]\n",
            " [ 219 1281]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Dissimilar       0.85      0.84      0.85      1500\n",
            "     Similar       0.84      0.85      0.85      1500\n",
            "\n",
            "    accuracy                           0.85      3000\n",
            "   macro avg       0.85      0.85      0.85      3000\n",
            "weighted avg       0.85      0.85      0.85      3000\n",
            "\n",
            "\n",
            "============================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "============================================================\n",
            "\n",
            "âœ“ Training curves saved to training_curves.png\n",
            "âœ“ ROC curves saved to roc_curves.png\n",
            "âœ“ Confusion matrices saved to confusion_matrices.png\n",
            "âœ“ Metrics comparison saved to metrics_comparison.png\n",
            "\n",
            "âœ“ Results table saved to results_table.csv\n",
            "\n",
            "============================================================\n",
            "QUANTITATIVE RESULTS SUMMARY\n",
            "============================================================\n",
            "            Model Accuracy Precision Recall F1 Score Roc Auc\n",
            "   BiLSTM-Siamese   0.9930    0.9920 0.9940   0.9930  0.9990\n",
            "Attention-Encoder   0.8463    0.8411 0.8540   0.8475  0.9276\n",
            "\n",
            "============================================================\n",
            "QUALITATIVE ANALYSIS - BiLSTM-Siamese\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "CORRECTLY MATCHED SIMILAR CLAUSES\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 1.0000):\n",
            "Clause 1: Termination Without Cause. (a) At any time Employer shall have the right to terminate the Term and the Executive's employment hereunder by written notice to the Executive. Any demotion resulting in a ...\n",
            "Clause 2: Termination Without Cause. If Executive's employment is terminated by the Company without Cause or if Executive resigns for Good Reason, then during the remainder of the Term commencing on the date of...\n",
            "\n",
            "Example 2 (Similarity Score: 1.0000):\n",
            "Clause 1: Insurance. Contractor shall maintain insurance coverage for work performed or services rendered under this Agreement as outlined and defined in the attached Special Provisions....\n",
            "Clause 2: Insurance. The Grantors, at their own expense, shall maintain or cause to be maintained insurance covering physical loss or damage to the Inventory and Equipment in accordance with Section 5.07 of the...\n",
            "\n",
            "Example 3 (Similarity Score: 1.0000):\n",
            "Clause 1: Fees and Expenses. In any arbitration pursuant to this Section 7(i), except as otherwise required by law, each party shall be responsible for the fees and expenses of its own attorneys and witnesses, ...\n",
            "Clause 2: Fees and Expenses. The Company agrees to pay its own expenses in connection with the preparation of this Agreement and performance of its obligations hereunder. The Company shall pay all stamp or othe...\n",
            "\n",
            "============================================================\n",
            "CORRECTLY MATCHED DISSIMILAR CLAUSES\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.0000):\n",
            "Clause 1: Financial Statements. The financial statements of the Company included in the Registration Statement, the Time of Sale Prospectus and the Prospectus, together with the related schedules and notes, as ...\n",
            "Clause 2: Adjustments. The Award is subject to adjustment in accordance with Section 4.3 of the Plan....\n",
            "\n",
            "Example 2 (Similarity Score: 0.0002):\n",
            "Clause 1: Additional Documents. The Purchaser shall have received such other documents as the Purchaser may reasonably request in good faith for the purpose of (i) evidencing the accuracy of any representation ...\n",
            "Clause 2: Investment Company Act. The Company is not an \"investment company\" or an entity \"controlled\" by an \"investment company\" as such terms are defined in the Investment Company Act of 1940, as amended (the...\n",
            "\n",
            "Example 3 (Similarity Score: 0.1869):\n",
            "Clause 1: Miscellaneous. This Agreement, including the exhibits attached hereto, represents the entire understanding of the parties hereto with respect to the subject matter hereof, supersedes any and all other...\n",
            "Clause 2: Execution. This Agreement may be executed by the Parties in any number of identical counterparts, each of which, for all purposes shall be deemed to be an original, and all of which shall constitute, ...\n",
            "\n",
            "============================================================\n",
            "INCORRECTLY MATCHED SIMILAR CLAUSES (Should be Similar)\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.1342):\n",
            "Clause 1: Person any individual, corporation, limited liability company, association, partnership, trust, organization, government, governmental agency, or other entity....\n",
            "Clause 2: Person. Any individual, corporation, partnership, joint venture, association, joint-stock company, limited liability company, trust, unincorporated organization or government or any agency or politica...\n",
            "\n",
            "Example 2 (Similarity Score: 0.0022):\n",
            "Clause 1: Transfer. Any Transfer occurs in violation of Section 8.1 of this Agreement....\n",
            "Clause 2: Transfer a. If an Event of Termination has occurred and is continuing, either the Trustee or Certificateholders with aggregate Percentage Interests representing 25% or more of the Trust, by notice in ...\n",
            "\n",
            "Example 3 (Similarity Score: 0.0022):\n",
            "Clause 1: Loans. The Revolving Credit Loans....\n",
            "Clause 2: Loans make any loans or grant any credit (save for normal trade credit in the ordinary course of business) to any person or agree to do so;...\n",
            "\n",
            "============================================================\n",
            "INCORRECTLY MATCHED DISSIMILAR CLAUSES (Should be Dissimilar)\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.8921):\n",
            "Clause 1: Modification and Waiver. The modification or waiver of any of Owner's Obligations or Lender's rights under this Agreement must be contained in a writing signed by Lender. Lender may perform any of Own...\n",
            "Clause 2: Reinstatement. 41 6.6 Remedies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 6.7 Subrogation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41...\n",
            "\n",
            "Example 2 (Similarity Score: 0.9276):\n",
            "Clause 1: Licenses. (a) Any Governmental Body shall (i) revoke, terminate, suspend or adversely modify any license or permit of any Loan Party, the continuation of which is material to the continuation of any L...\n",
            "Clause 2: Taxes. (a) Except as set forth in Section 3.14(a) of the Company Disclosure Schedule: (i) the Company and each of its Subsidiaries have timely filed all income Tax Returns and all other material Tax R...\n",
            "\n",
            "Example 3 (Similarity Score: 0.9166):\n",
            "Clause 1: Maintenance of Insurance. (a) Maintain with (i) financially sound and reputable insurance companies and (ii) insurance companies that are not Affiliates of the Borrower (other than Ashmont Insurance C...\n",
            "Clause 2: Severability of Provisions. If any provision of this Guaranty is for any reason held to be invalid, illegal or unenforceable in any respect, that provision shall not affect the validity, legality or e...\n",
            "\n",
            "============================================================\n",
            "QUALITATIVE ANALYSIS - Attention-Encoder\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "CORRECTLY MATCHED SIMILAR CLAUSES\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.9997):\n",
            "Clause 1: Termination Without Cause. (a) At any time Employer shall have the right to terminate the Term and the Executive's employment hereunder by written notice to the Executive. Any demotion resulting in a ...\n",
            "Clause 2: Termination Without Cause. If Executive's employment is terminated by the Company without Cause or if Executive resigns for Good Reason, then during the remainder of the Term commencing on the date of...\n",
            "\n",
            "Example 2 (Similarity Score: 0.9909):\n",
            "Clause 1: Insurance. Contractor shall maintain insurance coverage for work performed or services rendered under this Agreement as outlined and defined in the attached Special Provisions....\n",
            "Clause 2: Insurance. The Grantors, at their own expense, shall maintain or cause to be maintained insurance covering physical loss or damage to the Inventory and Equipment in accordance with Section 5.07 of the...\n",
            "\n",
            "Example 3 (Similarity Score: 1.0000):\n",
            "Clause 1: Fees and Expenses. In any arbitration pursuant to this Section 7(i), except as otherwise required by law, each party shall be responsible for the fees and expenses of its own attorneys and witnesses, ...\n",
            "Clause 2: Fees and Expenses. The Company agrees to pay its own expenses in connection with the preparation of this Agreement and performance of its obligations hereunder. The Company shall pay all stamp or othe...\n",
            "\n",
            "============================================================\n",
            "CORRECTLY MATCHED DISSIMILAR CLAUSES\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.0322):\n",
            "Clause 1: Financial Statements. The financial statements of the Company included in the Registration Statement, the Time of Sale Prospectus and the Prospectus, together with the related schedules and notes, as ...\n",
            "Clause 2: Adjustments. The Award is subject to adjustment in accordance with Section 4.3 of the Plan....\n",
            "\n",
            "Example 2 (Similarity Score: 0.0101):\n",
            "Clause 1: Additional Documents. The Purchaser shall have received such other documents as the Purchaser may reasonably request in good faith for the purpose of (i) evidencing the accuracy of any representation ...\n",
            "Clause 2: Investment Company Act. The Company is not an \"investment company\" or an entity \"controlled\" by an \"investment company\" as such terms are defined in the Investment Company Act of 1940, as amended (the...\n",
            "\n",
            "Example 3 (Similarity Score: 0.4209):\n",
            "Clause 1: Miscellaneous. This Agreement, including the exhibits attached hereto, represents the entire understanding of the parties hereto with respect to the subject matter hereof, supersedes any and all other...\n",
            "Clause 2: Execution. This Agreement may be executed by the Parties in any number of identical counterparts, each of which, for all purposes shall be deemed to be an original, and all of which shall constitute, ...\n",
            "\n",
            "============================================================\n",
            "INCORRECTLY MATCHED SIMILAR CLAUSES (Should be Similar)\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.2757):\n",
            "Clause 1: DURATION OF AGREEMENT. The parties hereto agree that this Agreement shall be effective from January 1, 2016 to December 31, 2018 and thereafter from year to year unless notice of desire to amend or te...\n",
            "Clause 2: DURATION OF AGREEMENT. Except as otherwise set forth herein, the rights and obligations of the Corporation and each Investor and Canadian Investor set forth herein shall survive indefinitely, unless a...\n",
            "\n",
            "Example 2 (Similarity Score: 0.1795):\n",
            "Clause 1: Enforceability. The covenants of this Article 7 are several and separate, and the unenforceability of any specific covenant shall not affect the provisions of any other covenant. If any provision of t...\n",
            "Clause 2: Enforceability. In the event any portion of this Agreement is found to be invalid or unenforceable, the remainder shall remain in full force and effect....\n",
            "\n",
            "Example 3 (Similarity Score: 0.1578):\n",
            "Clause 1: Applicable Law. The law of Georgia shall govern this Contract. In case any dispute or controversy arises between the Design Professional and the Owner, either party may exercise those legal remedies a...\n",
            "Clause 2: Applicable Law. Except to the extent preempted by federal law, the laws of the State of Indiana, without regard to that Stateâ€™s choice of law principles, shall govern this Agreement in all respects, w...\n",
            "\n",
            "============================================================\n",
            "INCORRECTLY MATCHED DISSIMILAR CLAUSES (Should be Dissimilar)\n",
            "============================================================\n",
            "\n",
            "Example 1 (Similarity Score: 0.9145):\n",
            "Clause 1: General. Subject to the terms and conditions set forth herein, the Borrower may request the issuance of Letters of Credit for its own account, in a form reasonably acceptable to the Administrative Age...\n",
            "Clause 2: Support. 1. The total amount of support that J&J Affiliate will provide for the Project amounts to...\n",
            "\n",
            "Example 2 (Similarity Score: 0.9973):\n",
            "Clause 1: Compliance with Law. To the Borrower's knowledge, the Borrower and each of the Real Property Assets are in compliance with all laws, rules, regulations, orders, judgments, writs and decrees, including...\n",
            "Clause 2: Procedure. Before terminating the Agreement or participation of one or more beneficiaries, the JU will formally notify the coordinator: - informing it of its intention to terminate and the reasons why...\n",
            "\n",
            "Example 3 (Similarity Score: 0.9660):\n",
            "Clause 1: SPECIAL TERMS AND CONDITIONS OF TRUST. The following special terms and conditions are hereby agreed to: A. The Securities initially deposited in the Trust pursuant to Section 2.01 of the Standard Term...\n",
            "Clause 2: Fractional Shares. No fraction of a share of Zapworld Common Stock will be issued at the Effective Time, but in lieu thereof, each holder of Zap Santa Cruz Stock who would otherwise be entitled to a f...\n",
            "\n",
            "============================================================\n",
            "COMPARATIVE ANALYSIS & DISCUSSION\n",
            "============================================================\n",
            "\n",
            "1. BiLSTM-Siamese Network:\n",
            "   Strengths:\n",
            "   - Effectively captures sequential dependencies in legal text\n",
            "   - Bidirectional processing provides context from both directions\n",
            "   - Shared weights ensure consistent encoding of clause pairs\n",
            "   Weaknesses:\n",
            "   - May struggle with very long clauses due to vanishing gradients\n",
            "   - Sequential processing limits parallelization\n",
            "\n",
            "2. Attention-based Encoder:\n",
            "   Strengths:\n",
            "   - Self-attention focuses on important legal terms and phrases\n",
            "   - Parallel processing of all tokens improves efficiency\n",
            "   - Better at capturing long-range dependencies\n",
            "   Weaknesses:\n",
            "   - Requires more data to learn effective attention patterns\n",
            "   - Quadratic complexity with sequence length\n",
            "\n",
            "============================================================\n",
            "EXECUTION COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "âœ“ Generated files:\n",
            "  - training_curves.png\n",
            "  - roc_curves.png\n",
            "  - confusion_matrices.png\n",
            "  - metrics_comparison.png\n",
            "  - results_table.csv\n",
            "  - *_best.pth (model checkpoints)\n",
            "\n",
            "âœ“ All files saved to: /content\n",
            "  You can download them from the Colab file browser\n",
            "  Or they will be automatically saved if you mounted Drive with proper permissions\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Legal Clause Similarity Detection System - Google Colab Version\n",
        "================================================================\n",
        "This script implements multiple baseline NLP architectures for identifying semantic\n",
        "similarity between legal clauses without using pretrained transformers.\n",
        "\n",
        "GOOGLE COLAB COMPATIBLE - Reads data from Google Drive\n",
        "\n",
        "Architectures Implemented:\n",
        "1. BiLSTM-based Siamese Network\n",
        "2. Attention-based Encoder Network\n",
        "\n",
        "Author: Deep Learning Assignment 2\n",
        "\n",
        "USAGE IN COLAB:\n",
        "1. Upload the 'archive' folder to your Google Drive\n",
        "2. Run this script - it will automatically mount Google Drive\n",
        "3. Update DRIVE_ARCHIVE_PATH if your archive folder is in a different location\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# NLP imports\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    roc_auc_score, roc_curve, auc, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# GOOGLE COLAB SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def setup_colab():\n",
        "    \"\"\"Setup Google Colab environment and mount Google Drive.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"GOOGLE COLAB SETUP\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Check if running in Colab\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"âœ“ Running in Google Colab\")\n",
        "\n",
        "        # Mount Google Drive\n",
        "        from google.colab import drive\n",
        "        print(\"\\nMounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"âœ“ Google Drive mounted successfully\")\n",
        "\n",
        "        return True, '/content/drive/MyDrive/DL_dataset'\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âš  Not running in Google Colab\")\n",
        "        print(\"Using local file system\")\n",
        "        return False, '.'\n",
        "\n",
        "# Check Colab and mount drive\n",
        "IN_COLAB, DRIVE_ROOT = setup_colab()\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - MODIFY THIS PATH IF NEEDED\n",
        "# ============================================================================\n",
        "\n",
        "# Default path - modify this if your archive folder is in a different location\n",
        "# Example paths:\n",
        "# - '/content/drive/MyDrive/archive'                    (archive in root of Drive)\n",
        "# - '/content/drive/MyDrive/DL/archive'                 (archive in DL folder)\n",
        "# - '/content/drive/MyDrive/FAST/DL/archive'           (archive in FAST/DL folder)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # MODIFY THIS PATH to match your Google Drive structure\n",
        "    DRIVE_ARCHIVE_PATH = os.path.join(DRIVE_ROOT, 'archive')\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Looking for archive folder at:\")\n",
        "    print(f\"  {DRIVE_ARCHIVE_PATH}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Check if the path exists\n",
        "    if not os.path.exists(DRIVE_ARCHIVE_PATH):\n",
        "        print(\"\\nâš  WARNING: Archive folder not found at the specified path!\")\n",
        "        print(\"\\nPlease update DRIVE_ARCHIVE_PATH in the script to match your folder location.\")\n",
        "        print(\"\\nYour Google Drive structure:\")\n",
        "\n",
        "        # Show what's in the Drive root\n",
        "        try:\n",
        "            items = os.listdir(DRIVE_ROOT)\n",
        "            print(f\"\\nContents of {DRIVE_ROOT}:\")\n",
        "            for item in items[:20]:  # Show first 20 items\n",
        "                item_path = os.path.join(DRIVE_ROOT, item)\n",
        "                if os.path.isdir(item_path):\n",
        "                    print(f\"  ðŸ“ {item}/\")\n",
        "                else:\n",
        "                    print(f\"  ðŸ“„ {item}\")\n",
        "            if len(items) > 20:\n",
        "                print(f\"  ... and {len(items) - 20} more items\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(\"\\nExample paths you might need:\")\n",
        "        print(\"  DRIVE_ARCHIVE_PATH = os.path.join(DRIVE_ROOT, 'archive')\")\n",
        "        print(\"  DRIVE_ARCHIVE_PATH = os.path.join(DRIVE_ROOT, 'DL', 'archive')\")\n",
        "        print(\"  DRIVE_ARCHIVE_PATH = os.path.join(DRIVE_ROOT, 'FAST', 'DL', 'archive')\")\n",
        "\n",
        "        # Try to find archive folder\n",
        "        print(\"\\nSearching for 'archive' folder...\")\n",
        "        for root, dirs, files in os.walk(DRIVE_ROOT):\n",
        "            if 'archive' in dirs:\n",
        "                found_path = os.path.join(root, 'archive')\n",
        "                # Count CSV files\n",
        "                csv_count = len(list(Path(found_path).glob('*.csv')))\n",
        "                if csv_count > 0:\n",
        "                    print(f\"  âœ“ Found: {found_path} ({csv_count} CSV files)\")\n",
        "\n",
        "        raise FileNotFoundError(\n",
        "            f\"Archive folder not found at {DRIVE_ARCHIVE_PATH}. \"\n",
        "            \"Please update DRIVE_ARCHIVE_PATH variable in the script.\"\n",
        "        )\n",
        "    else:\n",
        "        # Count CSV files\n",
        "        csv_files = list(Path(DRIVE_ARCHIVE_PATH).glob('*.csv'))\n",
        "        print(f\"âœ“ Archive folder found with {len(csv_files)} CSV files\")\n",
        "\n",
        "    DATA_DIR = DRIVE_ARCHIVE_PATH\n",
        "else:\n",
        "    # Local mode (not in Colab)\n",
        "    DATA_DIR = 'archive'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class LegalClauseDataLoader:\n",
        "    \"\"\"Loads and processes legal clause data from CSV files.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir=DATA_DIR):\n",
        "        self.data_dir = data_dir\n",
        "        self.clauses = []\n",
        "        self.clause_types = []\n",
        "\n",
        "    def load_data(self, max_files=None):\n",
        "        \"\"\"Load all CSV files from the archive directory.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"LOADING LEGAL CLAUSE DATASET\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Data directory: {self.data_dir}\")\n",
        "\n",
        "        csv_files = list(Path(self.data_dir).glob('*.csv'))\n",
        "        if max_files:\n",
        "            csv_files = csv_files[:max_files]\n",
        "\n",
        "        print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "        all_clauses = []\n",
        "        all_types = []\n",
        "\n",
        "        # Progress tracking\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        for csv_file in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file, encoding='utf-8')\n",
        "                if 'clause_text' in df.columns and 'clause_type' in df.columns:\n",
        "                    clauses = df['clause_text'].dropna().tolist()\n",
        "                    types = df['clause_type'].dropna().tolist()\n",
        "\n",
        "                    # Ensure equal length\n",
        "                    min_len = min(len(clauses), len(types))\n",
        "                    all_clauses.extend(clauses[:min_len])\n",
        "                    all_types.extend(types[:min_len])\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {csv_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        self.clauses = all_clauses\n",
        "        self.clause_types = all_types\n",
        "\n",
        "        print(f\"\\nâœ“ Loaded {len(self.clauses)} clauses\")\n",
        "        print(f\"âœ“ Number of unique clause types: {len(set(self.clause_types))}\")\n",
        "\n",
        "        return self.clauses, self.clause_types\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Handles text cleaning and preprocessing for legal clauses.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        \"\"\"Clean and normalize legal clause text.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep sentence structure\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\(\\)\\-]', '', text)\n",
        "\n",
        "        # Strip leading/trailing whitespace\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        \"\"\"Simple word tokenization.\"\"\"\n",
        "        return text.split()\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Builds and manages vocabulary for text data.\"\"\"\n",
        "\n",
        "    def __init__(self, max_vocab_size=50000, min_freq=2):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.word_freq = Counter()\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from list of texts.\"\"\"\n",
        "        print(\"\\nBuilding vocabulary...\")\n",
        "\n",
        "        preprocessor = TextPreprocessor()\n",
        "\n",
        "        # Count word frequencies\n",
        "        for text in texts:\n",
        "            cleaned = preprocessor.clean_text(text)\n",
        "            tokens = preprocessor.tokenize(cleaned)\n",
        "            self.word_freq.update(tokens)\n",
        "\n",
        "        # Add words to vocabulary based on frequency\n",
        "        vocab_words = [word for word, freq in self.word_freq.most_common(self.max_vocab_size)\n",
        "                       if freq >= self.min_freq]\n",
        "\n",
        "        for idx, word in enumerate(vocab_words, start=2):\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "        print(f\"âœ“ Vocabulary size: {len(self.word2idx)}\")\n",
        "        print(f\"  Most common words: {[w for w, _ in self.word_freq.most_common(10)]}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to list of indices.\"\"\"\n",
        "        preprocessor = TextPreprocessor()\n",
        "        cleaned = preprocessor.clean_text(text)\n",
        "        tokens = preprocessor.tokenize(cleaned)\n",
        "        return [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class ClausePairGenerator:\n",
        "    \"\"\"Generates positive and negative clause pairs for similarity learning.\"\"\"\n",
        "\n",
        "    def __init__(self, clauses, clause_types):\n",
        "        self.clauses = clauses\n",
        "        self.clause_types = clause_types\n",
        "        self.type_to_clauses = self._group_by_type()\n",
        "\n",
        "    def _group_by_type(self):\n",
        "        \"\"\"Group clauses by their type.\"\"\"\n",
        "        type_dict = {}\n",
        "        for clause, ctype in zip(self.clauses, self.clause_types):\n",
        "            if ctype not in type_dict:\n",
        "                type_dict[ctype] = []\n",
        "            type_dict[ctype].append(clause)\n",
        "        return type_dict\n",
        "\n",
        "    def generate_pairs(self, num_pairs=10000):\n",
        "        \"\"\"\n",
        "        Generate clause pairs with labels.\n",
        "        Label 1: Similar (same clause type)\n",
        "        Label 0: Dissimilar (different clause type)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"GENERATING CLAUSE PAIRS\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        pairs = []\n",
        "        labels = []\n",
        "\n",
        "        # Generate positive pairs (similar clauses)\n",
        "        num_positive = num_pairs // 2\n",
        "        types_list = list(self.type_to_clauses.keys())\n",
        "\n",
        "        for _ in range(num_positive):\n",
        "            # Select a random clause type\n",
        "            ctype = np.random.choice(types_list)\n",
        "            clauses_of_type = self.type_to_clauses[ctype]\n",
        "\n",
        "            # If there are at least 2 clauses of this type, create a pair\n",
        "            if len(clauses_of_type) >= 2:\n",
        "                clause1, clause2 = np.random.choice(clauses_of_type, size=2, replace=False)\n",
        "                pairs.append((clause1, clause2))\n",
        "                labels.append(1)  # Similar\n",
        "\n",
        "        # Generate negative pairs (dissimilar clauses)\n",
        "        num_negative = num_pairs - len(pairs)\n",
        "\n",
        "        for _ in range(num_negative):\n",
        "            # Select two different clause types\n",
        "            type1, type2 = np.random.choice(types_list, size=2, replace=False)\n",
        "            clause1 = np.random.choice(self.type_to_clauses[type1])\n",
        "            clause2 = np.random.choice(self.type_to_clauses[type2])\n",
        "            pairs.append((clause1, clause2))\n",
        "            labels.append(0)  # Dissimilar\n",
        "\n",
        "        print(f\"âœ“ Generated {len(pairs)} clause pairs\")\n",
        "        print(f\"  Positive pairs (similar): {sum(labels)}\")\n",
        "        print(f\"  Negative pairs (dissimilar): {len(labels) - sum(labels)}\")\n",
        "\n",
        "        return pairs, labels\n",
        "\n",
        "\n",
        "class ClausePairDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for clause pairs.\"\"\"\n",
        "\n",
        "    def __init__(self, pairs, labels, vocabulary, max_length=100):\n",
        "        self.pairs = pairs\n",
        "        self.labels = labels\n",
        "        self.vocabulary = vocabulary\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clause1, clause2 = self.pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Encode clauses\n",
        "        encoded1 = self.vocabulary.encode(clause1)[:self.max_length]\n",
        "        encoded2 = self.vocabulary.encode(clause2)[:self.max_length]\n",
        "\n",
        "        # Convert to tensors\n",
        "        tensor1 = torch.LongTensor(encoded1)\n",
        "        tensor2 = torch.LongTensor(encoded2)\n",
        "        label_tensor = torch.FloatTensor([label])\n",
        "\n",
        "        return tensor1, tensor2, label_tensor\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to pad sequences in a batch.\"\"\"\n",
        "    clause1_list, clause2_list, labels = [], [], []\n",
        "\n",
        "    for clause1, clause2, label in batch:\n",
        "        clause1_list.append(clause1)\n",
        "        clause2_list.append(clause2)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Pad sequences\n",
        "    clause1_padded = pad_sequence(clause1_list, batch_first=True, padding_value=0)\n",
        "    clause2_padded = pad_sequence(clause2_list, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels)\n",
        "\n",
        "    # Get lengths for packed sequences\n",
        "    lengths1 = torch.LongTensor([len(c) for c in clause1_list])\n",
        "    lengths2 = torch.LongTensor([len(c) for c in clause2_list])\n",
        "\n",
        "    return clause1_padded, clause2_padded, lengths1, lengths2, labels\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "\n",
        "class BiLSTMSiameseNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture 1: BiLSTM-based Siamese Network\n",
        "\n",
        "    This architecture uses a shared BiLSTM encoder for both clauses,\n",
        "    then computes similarity based on the encoded representations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(BiLSTMSiameseNetwork, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Similarity computation layers\n",
        "        lstm_output_dim = hidden_dim * 2  # Bidirectional\n",
        "        self.fc1 = nn.Linear(lstm_output_dim * 4, 256)  # Concatenated features\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def encode_clause(self, clause, lengths):\n",
        "        \"\"\"Encode a single clause using BiLSTM.\"\"\"\n",
        "        # Embedding\n",
        "        embedded = self.embedding(clause)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # BiLSTM\n",
        "        packed_output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # Unpack\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Use last hidden state (concatenate forward and backward)\n",
        "        # hidden shape: (num_layers * 2, batch, hidden_dim)\n",
        "        forward_hidden = hidden[-2, :, :]\n",
        "        backward_hidden = hidden[-1, :, :]\n",
        "        encoded = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def forward(self, clause1, clause2, lengths1, lengths2):\n",
        "        \"\"\"Forward pass for clause pair.\"\"\"\n",
        "        # Encode both clauses\n",
        "        encoded1 = self.encode_clause(clause1, lengths1)\n",
        "        encoded2 = self.encode_clause(clause2, lengths2)\n",
        "\n",
        "        # Compute similarity features\n",
        "        # Concatenate: [encoded1, encoded2, abs(encoded1-encoded2), encoded1*encoded2]\n",
        "        diff = torch.abs(encoded1 - encoded2)\n",
        "        prod = encoded1 * encoded2\n",
        "        combined = torch.cat([encoded1, encoded2, diff, prod], dim=1)\n",
        "\n",
        "        # Feed through fully connected layers\n",
        "        x = self.fc1(combined)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        output = torch.sigmoid(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class AttentionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture 2: Attention-based Encoder Network\n",
        "\n",
        "    Uses self-attention mechanism to capture important words in clauses,\n",
        "    then computes similarity based on attended representations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_heads=4, dropout=0.3):\n",
        "        super(AttentionEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Self-attention layers\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embedding_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embedding_dim)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Similarity computation\n",
        "        self.fc1 = nn.Linear(embedding_dim * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def create_attention_mask(self, lengths, max_len):\n",
        "        \"\"\"Create attention mask for padding.\"\"\"\n",
        "        batch_size = lengths.size(0)\n",
        "        mask = torch.arange(max_len).expand(batch_size, max_len).to(lengths.device)\n",
        "        mask = mask >= lengths.unsqueeze(1)\n",
        "        return mask\n",
        "\n",
        "    def encode_clause(self, clause, lengths):\n",
        "        \"\"\"Encode clause using self-attention.\"\"\"\n",
        "        # Embedding\n",
        "        embedded = self.embedding(clause)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Create attention mask\n",
        "        max_len = clause.size(1)\n",
        "        attn_mask = self.create_attention_mask(lengths, max_len)\n",
        "\n",
        "        # Self-attention with residual connection\n",
        "        attended, _ = self.attention(\n",
        "            embedded, embedded, embedded,\n",
        "            key_padding_mask=attn_mask\n",
        "        )\n",
        "        attended = self.layer_norm1(embedded + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ffn_output = self.ffn(attended)\n",
        "        output = self.layer_norm2(attended + self.dropout(ffn_output))\n",
        "\n",
        "        # Mean pooling (excluding padding)\n",
        "        mask = (~attn_mask).float().unsqueeze(-1)\n",
        "        masked_output = output * mask\n",
        "        summed = masked_output.sum(dim=1)\n",
        "        lengths_expanded = lengths.unsqueeze(-1).float()\n",
        "        encoded = summed / lengths_expanded\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def forward(self, clause1, clause2, lengths1, lengths2):\n",
        "        \"\"\"Forward pass for clause pair.\"\"\"\n",
        "        # Encode both clauses\n",
        "        encoded1 = self.encode_clause(clause1, lengths1)\n",
        "        encoded2 = self.encode_clause(clause2, lengths2)\n",
        "\n",
        "        # Compute similarity features\n",
        "        diff = torch.abs(encoded1 - encoded2)\n",
        "        prod = encoded1 * encoded2\n",
        "        combined = torch.cat([encoded1, encoded2, diff, prod], dim=1)\n",
        "\n",
        "        # Similarity scoring\n",
        "        x = self.fc1(combined)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        output = torch.sigmoid(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CNNBiLSTMHybrid(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture 3: CNN-BiLSTM Hybrid Network\n",
        "\n",
        "    Combines CNN for local feature extraction with BiLSTM for\n",
        "    sequential modeling, providing a comprehensive representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256,\n",
        "                 num_filters=128, filter_sizes=[3, 4, 5], num_layers=2, dropout=0.3):\n",
        "        super(CNNBiLSTMHybrid, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # CNN layers with multiple filter sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        # BiLSTM layer\n",
        "        cnn_output_dim = num_filters * len(filter_sizes)\n",
        "        self.lstm = nn.LSTM(\n",
        "            cnn_output_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Similarity computation\n",
        "        lstm_output_dim = hidden_dim * 2  # Bidirectional\n",
        "        self.fc1 = nn.Linear(lstm_output_dim * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def encode_clause(self, clause, lengths):\n",
        "        \"\"\"Encode clause using CNN-BiLSTM hybrid.\"\"\"\n",
        "        # Embedding\n",
        "        embedded = self.embedding(clause)  # (batch, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # CNN expects (batch, embedding_dim, seq_len)\n",
        "        embedded_transposed = embedded.transpose(1, 2)\n",
        "\n",
        "        # Apply multiple convolutional filters\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_out = F.relu(conv(embedded_transposed))  # (batch, num_filters, seq_len - kernel + 1)\n",
        "            # Transpose back to (batch, seq_len, num_filters)\n",
        "            conv_out = conv_out.transpose(1, 2)\n",
        "            conv_outputs.append(conv_out)\n",
        "\n",
        "        # Concatenate all CNN outputs\n",
        "        # Need to pad to same length\n",
        "        max_len = max(co.size(1) for co in conv_outputs)\n",
        "        padded_convs = []\n",
        "        for co in conv_outputs:\n",
        "            if co.size(1) < max_len:\n",
        "                padding = torch.zeros(co.size(0), max_len - co.size(1), co.size(2)).to(co.device)\n",
        "                co = torch.cat([co, padding], dim=1)\n",
        "            padded_convs.append(co)\n",
        "\n",
        "        cnn_output = torch.cat(padded_convs, dim=2)  # (batch, max_len, total_filters)\n",
        "        cnn_output = self.dropout(cnn_output)\n",
        "\n",
        "        # BiLSTM\n",
        "        # Adjust lengths for CNN output\n",
        "        adjusted_lengths = torch.clamp(lengths - max(self.convs[0].kernel_size[0] - 1 for _ in range(1)), min=1)\n",
        "\n",
        "        packed = pack_padded_sequence(\n",
        "            cnn_output[:, :cnn_output.size(1), :],\n",
        "            adjusted_lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        packed_output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # Use last hidden state\n",
        "        forward_hidden = hidden[-2, :, :]\n",
        "        backward_hidden = hidden[-1, :, :]\n",
        "        encoded = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def forward(self, clause1, clause2, lengths1, lengths2):\n",
        "        \"\"\"Forward pass for clause pair.\"\"\"\n",
        "        # Encode both clauses\n",
        "        encoded1 = self.encode_clause(clause1, lengths1)\n",
        "        encoded2 = self.encode_clause(clause2, lengths2)\n",
        "\n",
        "        # Compute similarity features\n",
        "        diff = torch.abs(encoded1 - encoded2)\n",
        "        prod = encoded1 * encoded2\n",
        "        combined = torch.cat([encoded1, encoded2, diff, prod], dim=1)\n",
        "\n",
        "        # Similarity scoring\n",
        "        x = self.fc1(combined)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        output = torch.sigmoid(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Handles model training and evaluation.\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, model_name=\"Model\"):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self, train_loader, optimizer, criterion):\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        for clause1, clause2, lengths1, lengths2, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "            clause1 = clause1.to(self.device)\n",
        "            clause2 = clause2.to(self.device)\n",
        "            lengths1 = lengths1.to(self.device)\n",
        "            lengths2 = lengths2.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(clause1, clause2, lengths1, lengths2)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            predicted = (outputs >= 0.5).float()\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def evaluate(self, val_loader, criterion):\n",
        "        \"\"\"Evaluate the model.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for clause1, clause2, lengths1, lengths2, labels in val_loader:\n",
        "                clause1 = clause1.to(self.device)\n",
        "                clause2 = clause2.to(self.device)\n",
        "                lengths1 = lengths1.to(self.device)\n",
        "                lengths2 = lengths2.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(clause1, clause2, lengths1, lengths2)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                predicted = (outputs >= 0.5).float()\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Store for metrics calculation\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return avg_loss, accuracy, all_predictions, all_labels, all_probs\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=20, learning_rate=0.001):\n",
        "        \"\"\"Train the model for multiple epochs.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TRAINING {self.model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        patience = 7\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Train\n",
        "            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc, _, _, _ = self.evaluate(val_loader, criterion)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            old_lr = optimizer.param_groups[0]['lr']\n",
        "            scheduler.step(val_loss)\n",
        "            new_lr = optimizer.param_groups[0]['lr']\n",
        "            if old_lr != new_lr:\n",
        "                print(f\"   Learning rate reduced: {old_lr:.6f} â†’ {new_lr:.6f}\")\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), f'{self.model_name}_best.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load(f'{self.model_name}_best.pth'))\n",
        "        print(f\"\\nâœ“ Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation with all required metrics.\"\"\"\n",
        "\n",
        "    def __init__(self, model, model_name, device):\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate_comprehensive(self, test_loader):\n",
        "        \"\"\"Compute all evaluation metrics.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EVALUATING {self.model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for clause1, clause2, lengths1, lengths2, labels in test_loader:\n",
        "                clause1 = clause1.to(self.device)\n",
        "                clause2 = clause2.to(self.device)\n",
        "                lengths1 = lengths1.to(self.device)\n",
        "                lengths2 = lengths2.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(clause1, clause2, lengths1, lengths2)\n",
        "                predicted = (outputs >= 0.5).float()\n",
        "\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(outputs.cpu().numpy())\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        all_predictions = np.array(all_predictions).flatten()\n",
        "        all_labels = np.array(all_labels).flatten()\n",
        "        all_probs = np.array(all_probs).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_predictions, average='binary', zero_division=0\n",
        "        )\n",
        "\n",
        "        # ROC-AUC\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "            fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "        except:\n",
        "            roc_auc = 0.0\n",
        "            fpr, tpr = None, None\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        # Store results\n",
        "        self.results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_predictions,\n",
        "            'labels': all_labels,\n",
        "            'probabilities': all_probs,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr\n",
        "        }\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall:    {recall:.4f}\")\n",
        "        print(f\"F1-Score:  {f1:.4f}\")\n",
        "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "        print(f\"\\nClassification Report:\")\n",
        "        print(classification_report(all_labels, all_predictions,\n",
        "                                   target_names=['Dissimilar', 'Similar']))\n",
        "\n",
        "        return self.results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION AND ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"Visualize and compare model results.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(trainers, save_path='training_curves.png'):\n",
        "        \"\"\"Plot training and validation curves for all models.\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        for trainer in trainers:\n",
        "            epochs = range(1, len(trainer.train_losses) + 1)\n",
        "            axes[0].plot(epochs, trainer.train_losses, label=f'{trainer.model_name} - Train', linestyle='--')\n",
        "            axes[0].plot(epochs, trainer.val_losses, label=f'{trainer.model_name} - Val')\n",
        "\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_title('Training and Validation Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy curves\n",
        "        for trainer in trainers:\n",
        "            epochs = range(1, len(trainer.train_accuracies) + 1)\n",
        "            axes[1].plot(epochs, trainer.train_accuracies, label=f'{trainer.model_name} - Train', linestyle='--')\n",
        "            axes[1].plot(epochs, trainer.val_accuracies, label=f'{trainer.model_name} - Val')\n",
        "\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Accuracy')\n",
        "        axes[1].set_title('Training and Validation Accuracy')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nâœ“ Training curves saved to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_roc_curves(evaluators, save_path='roc_curves.png'):\n",
        "        \"\"\"Plot ROC curves for all models.\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for evaluator in evaluators:\n",
        "            results = evaluator.results\n",
        "            if results['fpr'] is not None and results['tpr'] is not None:\n",
        "                plt.plot(results['fpr'], results['tpr'],\n",
        "                        label=f\"{evaluator.model_name} (AUC = {results['roc_auc']:.4f})\",\n",
        "                        linewidth=2)\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves - Model Comparison')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ“ ROC curves saved to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrices(evaluators, save_path='confusion_matrices.png'):\n",
        "        \"\"\"Plot confusion matrices for all models.\"\"\"\n",
        "        n_models = len(evaluators)\n",
        "        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "\n",
        "        if n_models == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for idx, evaluator in enumerate(evaluators):\n",
        "            cm = evaluator.results['confusion_matrix']\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                       xticklabels=['Dissimilar', 'Similar'],\n",
        "                       yticklabels=['Dissimilar', 'Similar'])\n",
        "            axes[idx].set_title(f'{evaluator.model_name}\\nConfusion Matrix')\n",
        "            axes[idx].set_ylabel('True Label')\n",
        "            axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ“ Confusion matrices saved to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_metrics_comparison(evaluators, save_path='metrics_comparison.png'):\n",
        "        \"\"\"Create bar plot comparing all metrics across models.\"\"\"\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "        model_names = [e.model_name for e in evaluators]\n",
        "\n",
        "        data = {metric: [e.results[metric] for e in evaluators] for metric in metrics}\n",
        "\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.25\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        for i, model_name in enumerate(model_names):\n",
        "            values = [data[metric][i] for metric in metrics]\n",
        "            ax.bar(x + i*width, values, width, label=model_name)\n",
        "\n",
        "        ax.set_xlabel('Metrics')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Model Performance Comparison')\n",
        "        ax.set_xticks(x + width)\n",
        "        ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'])\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        ax.set_ylim([0, 1.0])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"âœ“ Metrics comparison saved to {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def create_results_table(evaluators, save_path='results_table.csv'):\n",
        "        \"\"\"Create a comprehensive results table.\"\"\"\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "\n",
        "        results_dict = {'Model': [e.model_name for e in evaluators]}\n",
        "        for metric in metrics:\n",
        "            results_dict[metric.replace('_', ' ').title()] = [\n",
        "                f\"{e.results[metric]:.4f}\" for e in evaluators\n",
        "            ]\n",
        "\n",
        "        df = pd.DataFrame(results_dict)\n",
        "        df.to_csv(save_path, index=False)\n",
        "        print(f\"\\nâœ“ Results table saved to {save_path}\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"QUANTITATIVE RESULTS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(df.to_string(index=False))\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "class QualitativeAnalyzer:\n",
        "    \"\"\"Analyze qualitative results - correctly and incorrectly matched clauses.\"\"\"\n",
        "\n",
        "    def __init__(self, model, vocabulary, device, model_name):\n",
        "        self.model = model\n",
        "        self.vocabulary = vocabulary\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def analyze_predictions(self, test_pairs, test_labels, num_examples=5):\n",
        "        \"\"\"Show examples of correct and incorrect predictions.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"QUALITATIVE ANALYSIS - {self.model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        correct_similar = []\n",
        "        correct_dissimilar = []\n",
        "        incorrect_similar = []\n",
        "        incorrect_dissimilar = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, ((clause1, clause2), label) in enumerate(zip(test_pairs, test_labels)):\n",
        "                # Encode clauses\n",
        "                encoded1 = self.vocabulary.encode(clause1)[:100]\n",
        "                encoded2 = self.vocabulary.encode(clause2)[:100]\n",
        "\n",
        "                tensor1 = torch.LongTensor(encoded1).unsqueeze(0).to(self.device)\n",
        "                tensor2 = torch.LongTensor(encoded2).unsqueeze(0).to(self.device)\n",
        "                length1 = torch.LongTensor([len(encoded1)]).to(self.device)\n",
        "                length2 = torch.LongTensor([len(encoded2)]).to(self.device)\n",
        "\n",
        "                # Predict\n",
        "                output = self.model(tensor1, tensor2, length1, length2)\n",
        "                prediction = (output.item() >= 0.5)\n",
        "\n",
        "                # Categorize\n",
        "                if prediction == label:\n",
        "                    if label == 1:\n",
        "                        correct_similar.append((clause1, clause2, output.item()))\n",
        "                    else:\n",
        "                        correct_dissimilar.append((clause1, clause2, output.item()))\n",
        "                else:\n",
        "                    if label == 1:\n",
        "                        incorrect_similar.append((clause1, clause2, output.item()))\n",
        "                    else:\n",
        "                        incorrect_dissimilar.append((clause1, clause2, output.item()))\n",
        "\n",
        "                # Stop when we have enough examples\n",
        "                if (len(correct_similar) >= num_examples and\n",
        "                    len(correct_dissimilar) >= num_examples and\n",
        "                    len(incorrect_similar) >= num_examples and\n",
        "                    len(incorrect_dissimilar) >= num_examples):\n",
        "                    break\n",
        "\n",
        "        # Display examples\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CORRECTLY MATCHED SIMILAR CLAUSES\")\n",
        "        print(\"=\"*60)\n",
        "        for i, (c1, c2, score) in enumerate(correct_similar[:num_examples], 1):\n",
        "            print(f\"\\nExample {i} (Similarity Score: {score:.4f}):\")\n",
        "            print(f\"Clause 1: {c1[:200]}...\")\n",
        "            print(f\"Clause 2: {c2[:200]}...\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CORRECTLY MATCHED DISSIMILAR CLAUSES\")\n",
        "        print(\"=\"*60)\n",
        "        for i, (c1, c2, score) in enumerate(correct_dissimilar[:num_examples], 1):\n",
        "            print(f\"\\nExample {i} (Similarity Score: {score:.4f}):\")\n",
        "            print(f\"Clause 1: {c1[:200]}...\")\n",
        "            print(f\"Clause 2: {c2[:200]}...\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"INCORRECTLY MATCHED SIMILAR CLAUSES (Should be Similar)\")\n",
        "        print(\"=\"*60)\n",
        "        for i, (c1, c2, score) in enumerate(incorrect_similar[:num_examples], 1):\n",
        "            print(f\"\\nExample {i} (Similarity Score: {score:.4f}):\")\n",
        "            print(f\"Clause 1: {c1[:200]}...\")\n",
        "            print(f\"Clause 2: {c2[:200]}...\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"INCORRECTLY MATCHED DISSIMILAR CLAUSES (Should be Dissimilar)\")\n",
        "        print(\"=\"*60)\n",
        "        for i, (c1, c2, score) in enumerate(incorrect_dissimilar[:num_examples], 1):\n",
        "            print(f\"\\nExample {i} (Similarity Score: {score:.4f}):\")\n",
        "            print(f\"Clause 1: {c1[:200]}...\")\n",
        "            print(f\"Clause 2: {c2[:200]}...\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LEGAL CLAUSE SIMILARITY DETECTION SYSTEM\")\n",
        "    print(\"Deep Learning Assignment 2 - Google Colab Version\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Configuration\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_PAIRS = 20000\n",
        "    MAX_VOCAB_SIZE = 30000\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM = 256\n",
        "\n",
        "    # Step 1: Load and preprocess data\n",
        "    data_loader = LegalClauseDataLoader(data_dir=DATA_DIR)\n",
        "    clauses, clause_types = data_loader.load_data()\n",
        "\n",
        "    # Step 2: Build vocabulary\n",
        "    vocabulary = Vocabulary(max_vocab_size=MAX_VOCAB_SIZE, min_freq=2)\n",
        "    vocabulary.build_vocab(clauses)\n",
        "\n",
        "    # Step 3: Generate clause pairs\n",
        "    pair_generator = ClausePairGenerator(clauses, clause_types)\n",
        "    pairs, labels = pair_generator.generate_pairs(num_pairs=NUM_PAIRS)\n",
        "\n",
        "    # Step 4: Split data\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SPLITTING DATA\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Split into train, validation, and test sets\n",
        "    train_pairs, temp_pairs, train_labels, temp_labels = train_test_split(\n",
        "        pairs, labels, test_size=0.3, random_state=SEED, stratify=labels\n",
        "    )\n",
        "    val_pairs, test_pairs, val_labels, test_labels = train_test_split(\n",
        "        temp_pairs, temp_labels, test_size=0.5, random_state=SEED, stratify=temp_labels\n",
        "    )\n",
        "\n",
        "    print(f\"âœ“ Training set: {len(train_pairs)} pairs\")\n",
        "    print(f\"âœ“ Validation set: {len(val_pairs)} pairs\")\n",
        "    print(f\"âœ“ Test set: {len(test_pairs)} pairs\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ClausePairDataset(train_pairs, train_labels, vocabulary)\n",
        "    val_dataset = ClausePairDataset(val_pairs, val_labels, vocabulary)\n",
        "    test_dataset = ClausePairDataset(test_pairs, test_labels, vocabulary)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                             collate_fn=collate_fn, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                           collate_fn=collate_fn, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                            collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "    # Step 5: Initialize models\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"INITIALIZING MODELS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    vocab_size = len(vocabulary)\n",
        "\n",
        "    model1 = BiLSTMSiameseNetwork(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=2,\n",
        "        dropout=0.3\n",
        "    )\n",
        "    print(f\"\\n1. BiLSTM Siamese Network initialized\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model1.parameters()):,}\")\n",
        "\n",
        "    model2 = AttentionEncoder(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_heads=4,\n",
        "        dropout=0.3\n",
        "    )\n",
        "    print(f\"\\n2. Attention-based Encoder initialized\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model2.parameters()):,}\")\n",
        "\n",
        "    # Step 6: Train models\n",
        "    trainer1 = ModelTrainer(model1, device, model_name=\"BiLSTM-Siamese\")\n",
        "    trainer1.train(train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "\n",
        "    trainer2 = ModelTrainer(model2, device, model_name=\"Attention-Encoder\")\n",
        "    trainer2.train(train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
        "\n",
        "    trainers = [trainer1, trainer2]\n",
        "\n",
        "    # Step 7: Evaluate models\n",
        "    evaluator1 = ModelEvaluator(model1, \"BiLSTM-Siamese\", device)\n",
        "    results1 = evaluator1.evaluate_comprehensive(test_loader)\n",
        "\n",
        "    evaluator2 = ModelEvaluator(model2, \"Attention-Encoder\", device)\n",
        "    results2 = evaluator2.evaluate_comprehensive(test_loader)\n",
        "\n",
        "    evaluators = [evaluator1, evaluator2]\n",
        "\n",
        "    # Step 8: Visualize results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    visualizer = ResultsVisualizer()\n",
        "    visualizer.plot_training_curves(trainers)\n",
        "    visualizer.plot_roc_curves(evaluators)\n",
        "    visualizer.plot_confusion_matrices(evaluators)\n",
        "    visualizer.plot_metrics_comparison(evaluators)\n",
        "    visualizer.create_results_table(evaluators)\n",
        "\n",
        "    # Step 9: Qualitative analysis\n",
        "    for evaluator in evaluators:\n",
        "        analyzer = QualitativeAnalyzer(\n",
        "            evaluator.model, vocabulary, device, evaluator.model_name\n",
        "        )\n",
        "        analyzer.analyze_predictions(test_pairs, test_labels, num_examples=3)\n",
        "\n",
        "    # Step 10: Comparative analysis\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPARATIVE ANALYSIS & DISCUSSION\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(\"\\n1. BiLSTM-Siamese Network:\")\n",
        "    print(\"   Strengths:\")\n",
        "    print(\"   - Effectively captures sequential dependencies in legal text\")\n",
        "    print(\"   - Bidirectional processing provides context from both directions\")\n",
        "    print(\"   - Shared weights ensure consistent encoding of clause pairs\")\n",
        "    print(\"   Weaknesses:\")\n",
        "    print(\"   - May struggle with very long clauses due to vanishing gradients\")\n",
        "    print(\"   - Sequential processing limits parallelization\")\n",
        "\n",
        "    print(\"\\n2. Attention-based Encoder:\")\n",
        "    print(\"   Strengths:\")\n",
        "    print(\"   - Self-attention focuses on important legal terms and phrases\")\n",
        "    print(\"   - Parallel processing of all tokens improves efficiency\")\n",
        "    print(\"   - Better at capturing long-range dependencies\")\n",
        "    print(\"   Weaknesses:\")\n",
        "    print(\"   - Requires more data to learn effective attention patterns\")\n",
        "    print(\"   - Quadratic complexity with sequence length\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nâœ“ Generated files:\")\n",
        "    print(\"  - training_curves.png\")\n",
        "    print(\"  - roc_curves.png\")\n",
        "    print(\"  - confusion_matrices.png\")\n",
        "    print(\"  - metrics_comparison.png\")\n",
        "    print(\"  - results_table.csv\")\n",
        "    print(\"  - *_best.pth (model checkpoints)\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(f\"\\nâœ“ All files saved to: {os.getcwd()}\")\n",
        "        print(\"  You can download them from the Colab file browser\")\n",
        "        print(\"  Or they will be automatically saved if you mounted Drive with proper permissions\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}